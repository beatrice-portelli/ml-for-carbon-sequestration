{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31b4cb8-f146-48cc-8f80-5b4f9422c958",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- [1 / 104]\n",
      "File already exists. Skipping CS--Conf1--BaggedDT\n",
      "---------------- [2 / 104]\n",
      "File already exists. Skipping CSE--Conf1--BaggedDT\n",
      "---------------- [3 / 104]\n",
      "File already exists. Skipping CS--Conf2--BaggedDT\n",
      "---------------- [4 / 104]\n",
      "File already exists. Skipping CSE--Conf2--BaggedDT\n",
      "---------------- [5 / 104]\n",
      "File already exists. Skipping CS--Conf3--BaggedDT\n",
      "---------------- [6 / 104]\n",
      "File already exists. Skipping CSE--Conf3--BaggedDT\n",
      "---------------- [7 / 104]\n",
      "File already exists. Skipping CS--Conf4--BaggedDT\n",
      "---------------- [8 / 104]\n",
      "File already exists. Skipping CSE--Conf4--BaggedDT\n",
      "---------------- [9 / 104]\n",
      "File already exists. Skipping CS--Conf1--AdaBoost\n",
      "---------------- [10 / 104]\n",
      "File already exists. Skipping CSE--Conf1--AdaBoost\n",
      "---------------- [11 / 104]\n",
      "File already exists. Skipping CS--Conf2--AdaBoost\n",
      "---------------- [12 / 104]\n",
      "File already exists. Skipping CSE--Conf2--AdaBoost\n",
      "---------------- [13 / 104]\n",
      "File already exists. Skipping CS--Conf3--AdaBoost\n",
      "---------------- [14 / 104]\n",
      "File already exists. Skipping CSE--Conf3--AdaBoost\n",
      "---------------- [15 / 104]\n",
      "File already exists. Skipping CS--Conf4--AdaBoost\n",
      "---------------- [16 / 104]\n",
      "File already exists. Skipping CSE--Conf4--AdaBoost\n",
      "---------------- [17 / 104]\n",
      "File already exists. Skipping CS--Conf1--LightGBM\n",
      "---------------- [18 / 104]\n",
      "File already exists. Skipping CSE--Conf1--LightGBM\n",
      "---------------- [19 / 104]\n",
      "File already exists. Skipping CS--Conf2--LightGBM\n",
      "---------------- [20 / 104]\n",
      "File already exists. Skipping CSE--Conf2--LightGBM\n",
      "---------------- [21 / 104]\n",
      "File already exists. Skipping CS--Conf3--LightGBM\n",
      "---------------- [22 / 104]\n",
      "File already exists. Skipping CSE--Conf3--LightGBM\n",
      "---------------- [23 / 104]\n",
      "File already exists. Skipping CS--Conf4--LightGBM\n",
      "---------------- [24 / 104]\n",
      "File already exists. Skipping CSE--Conf4--LightGBM\n",
      "---------------- [25 / 104]\n",
      "File already exists. Skipping CS--Conf1--StackEns\n",
      "---------------- [26 / 104]\n",
      "File already exists. Skipping CSE--Conf1--StackEns\n",
      "---------------- [27 / 104]\n",
      "File already exists. Skipping CS--Conf2--StackEns\n",
      "---------------- [28 / 104]\n",
      "File already exists. Skipping CSE--Conf2--StackEns\n",
      "---------------- [29 / 104]\n",
      "File already exists. Skipping CS--Conf3--StackEns\n",
      "---------------- [30 / 104]\n",
      "File already exists. Skipping CSE--Conf3--StackEns\n",
      "---------------- [31 / 104]\n",
      "File already exists. Skipping CS--Conf4--StackEns\n",
      "---------------- [32 / 104]\n",
      "File already exists. Skipping CSE--Conf4--StackEns\n",
      "---------------- [33 / 104]\n",
      "Running CS--Conf1--GaussProc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/HDD/bportelli/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 9 is smaller than n_iter=30. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters are  {'kernel': None, 'alpha': 1e-10}\n",
      "The best score is  -9657.124406099047\n",
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CS--Conf1--GaussProc.pickle\n",
      "        R2   -1.59 ±  0.17\n",
      "      MAPE     1.0 ±   0.0\n",
      "      RMSE   98.21 ±  6.74\n",
      "     NRMSE  128.01 ± 10.68\n",
      "---------------- [34 / 104]\n",
      "Running CSE--Conf1--GaussProc\n",
      "The best hyperparameters are  {'kernel': None, 'alpha': 1e-10}\n",
      "The best score is  -4.268355698066373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/HDD/bportelli/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 9 is smaller than n_iter=30. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CSE--Conf1--GaussProc.pickle\n",
      "        R2    -2.0 ±  0.27\n",
      "      MAPE     1.0 ±   0.0\n",
      "      RMSE    2.06 ±  0.17\n",
      "     NRMSE  122.85 ± 12.69\n",
      "---------------- [35 / 104]\n",
      "Running CS--Conf2--GaussProc\n",
      "The best hyperparameters are  {'kernel': None, 'alpha': 1e-10}\n",
      "The best score is  -9657.124406099047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/HDD/bportelli/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 9 is smaller than n_iter=30. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CS--Conf2--GaussProc.pickle\n",
      "        R2   -1.59 ±  0.17\n",
      "      MAPE     1.0 ±   0.0\n",
      "      RMSE   98.21 ±  6.74\n",
      "     NRMSE  128.01 ± 10.68\n",
      "---------------- [36 / 104]\n",
      "Running CSE--Conf2--GaussProc\n",
      "The best hyperparameters are  {'kernel': None, 'alpha': 1e-10}\n",
      "The best score is  -4.268355698066373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/HDD/bportelli/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 9 is smaller than n_iter=30. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CSE--Conf2--GaussProc.pickle\n",
      "        R2    -2.0 ±  0.27\n",
      "      MAPE     1.0 ±   0.0\n",
      "      RMSE    2.06 ±  0.17\n",
      "     NRMSE  122.85 ± 12.69\n",
      "---------------- [37 / 104]\n",
      "Running CS--Conf3--GaussProc\n",
      "The best hyperparameters are  {'kernel': None, 'alpha': 1e-10}\n",
      "The best score is  -9657.124406099047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/HDD/bportelli/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 9 is smaller than n_iter=30. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CS--Conf3--GaussProc.pickle\n",
      "        R2   -1.59 ±  0.17\n",
      "      MAPE     1.0 ±   0.0\n",
      "      RMSE   98.21 ±  6.74\n",
      "     NRMSE  128.01 ± 10.68\n",
      "---------------- [38 / 104]\n",
      "Running CSE--Conf3--GaussProc\n",
      "The best hyperparameters are  {'kernel': None, 'alpha': 1e-10}\n",
      "The best score is  -4.268355698066373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/HDD/bportelli/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 9 is smaller than n_iter=30. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CSE--Conf3--GaussProc.pickle\n",
      "        R2    -2.0 ±  0.27\n",
      "      MAPE     1.0 ±   0.0\n",
      "      RMSE    2.06 ±  0.17\n",
      "     NRMSE  122.85 ± 12.69\n",
      "---------------- [39 / 104]\n",
      "Running CS--Conf4--GaussProc\n",
      "The best hyperparameters are  {'kernel': None, 'alpha': 1e-10}\n",
      "The best score is  -9657.124406099047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/HDD/bportelli/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 9 is smaller than n_iter=30. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CS--Conf4--GaussProc.pickle\n",
      "        R2   -1.59 ±  0.17\n",
      "      MAPE     1.0 ±   0.0\n",
      "      RMSE   98.21 ±  6.74\n",
      "     NRMSE  128.01 ± 10.68\n",
      "---------------- [40 / 104]\n",
      "Running CSE--Conf4--GaussProc\n",
      "The best hyperparameters are  {'kernel': None, 'alpha': 1e-10}\n",
      "The best score is  -4.268355698066373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/HDD/bportelli/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 9 is smaller than n_iter=30. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CSE--Conf4--GaussProc.pickle\n",
      "        R2    -2.0 ±  0.27\n",
      "      MAPE     1.0 ±   0.0\n",
      "      RMSE    2.06 ±  0.17\n",
      "     NRMSE  122.85 ± 12.69\n",
      "---------------- [41 / 104]\n",
      "File already exists. Skipping CS--Conf1--BayesianNN\n",
      "---------------- [42 / 104]\n",
      "File already exists. Skipping CSE--Conf1--BayesianNN\n",
      "---------------- [43 / 104]\n",
      "File already exists. Skipping CS--Conf2--BayesianNN\n",
      "---------------- [44 / 104]\n",
      "File already exists. Skipping CSE--Conf2--BayesianNN\n",
      "---------------- [45 / 104]\n",
      "File already exists. Skipping CS--Conf3--BayesianNN\n",
      "---------------- [46 / 104]\n",
      "File already exists. Skipping CSE--Conf3--BayesianNN\n",
      "---------------- [47 / 104]\n",
      "File already exists. Skipping CS--Conf4--BayesianNN\n",
      "---------------- [48 / 104]\n",
      "File already exists. Skipping CSE--Conf4--BayesianNN\n",
      "---------------- [49 / 104]\n",
      "File already exists. Skipping CS--Conf1--MLP\n",
      "---------------- [50 / 104]\n",
      "File already exists. Skipping CSE--Conf1--MLP\n",
      "---------------- [51 / 104]\n",
      "File already exists. Skipping CS--Conf2--MLP\n",
      "---------------- [52 / 104]\n",
      "File already exists. Skipping CSE--Conf2--MLP\n",
      "---------------- [53 / 104]\n",
      "File already exists. Skipping CS--Conf3--MLP\n",
      "---------------- [54 / 104]\n",
      "File already exists. Skipping CSE--Conf3--MLP\n",
      "---------------- [55 / 104]\n",
      "File already exists. Skipping CS--Conf4--MLP\n",
      "---------------- [56 / 104]\n",
      "File already exists. Skipping CSE--Conf4--MLP\n",
      "---------------- [57 / 104]\n",
      "File already exists. Skipping CS--Conf1--SVR\n",
      "---------------- [58 / 104]\n",
      "File already exists. Skipping CSE--Conf1--SVR\n",
      "---------------- [59 / 104]\n",
      "File already exists. Skipping CS--Conf2--SVR\n",
      "---------------- [60 / 104]\n",
      "File already exists. Skipping CSE--Conf2--SVR\n",
      "---------------- [61 / 104]\n",
      "File already exists. Skipping CS--Conf3--SVR\n",
      "---------------- [62 / 104]\n",
      "File already exists. Skipping CSE--Conf3--SVR\n",
      "---------------- [63 / 104]\n",
      "File already exists. Skipping CS--Conf4--SVR\n",
      "---------------- [64 / 104]\n",
      "File already exists. Skipping CSE--Conf4--SVR\n",
      "---------------- [65 / 104]\n",
      "File already exists. Skipping CS--Conf1--KNN\n",
      "---------------- [66 / 104]\n",
      "File already exists. Skipping CSE--Conf1--KNN\n",
      "---------------- [67 / 104]\n",
      "File already exists. Skipping CS--Conf2--KNN\n",
      "---------------- [68 / 104]\n",
      "File already exists. Skipping CSE--Conf2--KNN\n",
      "---------------- [69 / 104]\n",
      "File already exists. Skipping CS--Conf3--KNN\n",
      "---------------- [70 / 104]\n",
      "File already exists. Skipping CSE--Conf3--KNN\n",
      "---------------- [71 / 104]\n",
      "File already exists. Skipping CS--Conf4--KNN\n",
      "---------------- [72 / 104]\n",
      "File already exists. Skipping CSE--Conf4--KNN\n",
      "---------------- [73 / 104]\n",
      "File already exists. Skipping CS--Conf1--RF\n",
      "---------------- [74 / 104]\n",
      "File already exists. Skipping CSE--Conf1--RF\n",
      "---------------- [75 / 104]\n",
      "File already exists. Skipping CS--Conf2--RF\n",
      "---------------- [76 / 104]\n",
      "File already exists. Skipping CSE--Conf2--RF\n",
      "---------------- [77 / 104]\n",
      "File already exists. Skipping CS--Conf3--RF\n",
      "---------------- [78 / 104]\n",
      "File already exists. Skipping CSE--Conf3--RF\n",
      "---------------- [79 / 104]\n",
      "File already exists. Skipping CS--Conf4--RF\n",
      "---------------- [80 / 104]\n",
      "File already exists. Skipping CSE--Conf4--RF\n",
      "---------------- [81 / 104]\n",
      "File already exists. Skipping CS--Conf1--GBDT\n",
      "---------------- [82 / 104]\n",
      "File already exists. Skipping CSE--Conf1--GBDT\n",
      "---------------- [83 / 104]\n",
      "File already exists. Skipping CS--Conf2--GBDT\n",
      "---------------- [84 / 104]\n",
      "File already exists. Skipping CSE--Conf2--GBDT\n",
      "---------------- [85 / 104]\n",
      "File already exists. Skipping CS--Conf3--GBDT\n",
      "---------------- [86 / 104]\n",
      "File already exists. Skipping CSE--Conf3--GBDT\n",
      "---------------- [87 / 104]\n",
      "File already exists. Skipping CS--Conf4--GBDT\n",
      "---------------- [88 / 104]\n",
      "File already exists. Skipping CSE--Conf4--GBDT\n",
      "---------------- [89 / 104]\n",
      "File already exists. Skipping CS--Conf1--CatBoost\n",
      "---------------- [90 / 104]\n",
      "File already exists. Skipping CSE--Conf1--CatBoost\n",
      "---------------- [91 / 104]\n",
      "File already exists. Skipping CS--Conf2--CatBoost\n",
      "---------------- [92 / 104]\n",
      "File already exists. Skipping CSE--Conf2--CatBoost\n",
      "---------------- [93 / 104]\n",
      "File already exists. Skipping CS--Conf3--CatBoost\n",
      "---------------- [94 / 104]\n",
      "File already exists. Skipping CSE--Conf3--CatBoost\n",
      "---------------- [95 / 104]\n",
      "File already exists. Skipping CS--Conf4--CatBoost\n",
      "---------------- [96 / 104]\n",
      "File already exists. Skipping CSE--Conf4--CatBoost\n",
      "---------------- [97 / 104]\n",
      "File already exists. Skipping CS--Conf1--XGBoost\n",
      "---------------- [98 / 104]\n",
      "File already exists. Skipping CSE--Conf1--XGBoost\n",
      "---------------- [99 / 104]\n",
      "File already exists. Skipping CS--Conf2--XGBoost\n",
      "---------------- [100 / 104]\n",
      "File already exists. Skipping CSE--Conf2--XGBoost\n",
      "---------------- [101 / 104]\n",
      "File already exists. Skipping CS--Conf3--XGBoost\n",
      "---------------- [102 / 104]\n",
      "File already exists. Skipping CSE--Conf3--XGBoost\n",
      "---------------- [103 / 104]\n",
      "File already exists. Skipping CS--Conf4--XGBoost\n",
      "---------------- [104 / 104]\n",
      "File already exists. Skipping CSE--Conf4--XGBoost\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, LeaveOneOut\n",
    "from sklearn.linear_model import (Ridge, BayesianRidge)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "\n",
    "def normalized_root_mean_squared_error(y_true, y_pred, norm_factor=None):\n",
    "    if norm_factor is None:\n",
    "        assert False, \"Set norm_factor (for example the average target value for the training set)\"\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    return (rmse / norm_factor)*100\n",
    "\n",
    "model2paper = {\n",
    "    'CatBoostRegressor':         \"CatBoost\",\n",
    "    'GradientBoostingRegressor': \"GBDT\",\n",
    "    'KNeighborsRegressor':       \"KNN\",\n",
    "    'MLPRegressor':              \"MLP\",\n",
    "    'RandomForestRegressor':     \"RF\",\n",
    "    'SVR':                       \"SVR\",\n",
    "    'XGBRegressor':              \"XGBoost\",\n",
    "    # =========================================\n",
    "    \"GaussianProcessRegressor\": \"GaussProc\",\n",
    "    \"BayesianRidge\": \"BayesianNN\",\n",
    "    \"StackingRegressor\": \"StackEns\",\n",
    "    \"LGBMRegressor\": \"LightGBM\",\n",
    "    \"AdaBoostRegressor\": \"AdaBoost\",\n",
    "    \"BaggingRegressor\": \"BaggedDT\",\n",
    "}\n",
    "\n",
    "from configs import *\n",
    "from experiments_to_run import MODELS, CONFIGS, TARGETS\n",
    "\n",
    "if not os.path.exists(\"../results\"):\n",
    "    os.makedirs(\"../results\")\n",
    "if not os.path.exists(\"../figures_and_tables\"):\n",
    "    os.makedirs(\"../figures_and_tables\")\n",
    "\n",
    "OVERWRITE = False\n",
    "\n",
    "total_iterations = len(MODELS)*len(CONFIGS)*len(TARGETS)\n",
    "curr_iteration = 0\n",
    "\n",
    "for MODEL in MODELS:\n",
    "    for CONFIG in CONFIGS:\n",
    "        for TARGET in TARGETS:\n",
    "            \n",
    "            curr_iteration+=1\n",
    "            print(f\"---------------- [{curr_iteration} / {total_iterations}]\")\n",
    "            \n",
    "            if not OVERWRITE:\n",
    "                conf = f\"{TARGET}--{CONFIG.name}--{model2paper[MODEL[0](_).__class__.__name__]}\"\n",
    "                save_path = f\"../results/metrics--{conf}.pickle\"\n",
    "                if os.path.exists(save_path):\n",
    "                    print(\"File already exists. Skipping\", conf)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Running\", conf)\n",
    "\n",
    "            # fixing random seed as soon as possible\n",
    "            # for reproducibility\n",
    "            np.random.seed(123)\n",
    "            random.seed(123)\n",
    "\n",
    "            df = pd.read_csv(\"../data.csv\")\n",
    "            X = df[CONFIG.features]\n",
    "            y = df[TARGET]\n",
    "\n",
    "            # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "            model_class, param_distributions, search_cv_args = MODEL\n",
    "            \n",
    "            if param_distributions is None:\n",
    "                \n",
    "                regressor = model_class(_)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                search_cv = RandomizedSearchCV(\n",
    "                    model_class(_), param_distributions=param_distributions,\n",
    "                    scoring=\"neg_mean_squared_error\", random_state=0,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1,\n",
    "                    n_iter=30,\n",
    "                    return_train_score=True,\n",
    "                    **search_cv_args\n",
    "                )\n",
    "                search_cv.fit(X.values, y.values)\n",
    "\n",
    "                print(\"The best hyperparameters are \",search_cv.best_params_)\n",
    "                print(\"The best score is \",search_cv.best_score_)\n",
    "                \n",
    "                if type(search_cv.estimator) not in [\n",
    "                    KNeighborsRegressor,\n",
    "                    SVR,\n",
    "                    StackingRegressor,\n",
    "                    BayesianRidge,\n",
    "                ]:\n",
    "                    regressor = model_class(_).set_params( # use search_cv.estimator, to make it independent from the estimator's class\n",
    "                        random_state=0,           # fixed random state\n",
    "                        **search_cv.best_params_, # pass all parameters without to need to manually assign them\n",
    "                    )\n",
    "                else:\n",
    "                    regressor = model_class(_).set_params( # use search_cv.estimator, to make it independent from the estimator's class\n",
    "                        **search_cv.best_params_, # pass all parameters without to need to manually assign them\n",
    "                    )\n",
    "\n",
    "            data = []\n",
    "            \n",
    "#             # =================================================================\n",
    "#             # the following code is an alternative, to split the dataset in\n",
    "#             # 66/33 random train/test splits for 10 times to run the evaluation\n",
    "            \n",
    "#             folds = []\n",
    "\n",
    "#             for random_state in range(10):\n",
    "#                 cv = KFold(n_splits=3, random_state=random_state, shuffle=True)\n",
    "#                 tmp_folds = cv.split(X)\n",
    "#                 folds.append(next(tmp_folds))\n",
    "\n",
    "            # =================================================================\n",
    "            # 5-fold cross validation\n",
    "            \n",
    "            cv = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "            folds = cv.split(X)\n",
    "\n",
    "            for i, (train_index, test_index) in enumerate(folds):\n",
    "\n",
    "                X_train = X.iloc[train_index]\n",
    "                y_train = y.iloc[train_index].values\n",
    "\n",
    "                X_test = X.iloc[test_index]\n",
    "                y_test = y.iloc[test_index].values\n",
    "\n",
    "                regressor.fit(X_train.values, y_train)\n",
    "                y_pred = regressor.predict(X_test.values)\n",
    "\n",
    "                data.append({\n",
    "                    \"target\": TARGET,\n",
    "                    \"config\": CONFIG.name,\n",
    "                    \"model_name\": regressor.__class__.__name__,\n",
    "                    \"model\": model2paper[regressor.__class__.__name__],\n",
    "                    \"hyperparams\": None if param_distributions is None else search_cv.best_params_,\n",
    "                    \"fold\": i,\n",
    "                    \"X_train\": X_train,\n",
    "                    \"y_train\": y_train,\n",
    "                    \"X_test\": X_test,\n",
    "                    \"y_test\": y_test,\n",
    "                    \"y_pred\": y_pred,\n",
    "                    \"model_obj\": deepcopy(regressor),\n",
    "                })\n",
    "\n",
    "            data = pd.DataFrame(data)\n",
    "            save_path = f\"../results/predictions--{TARGET}--{CONFIG.name}--{model2paper[regressor.__class__.__name__]}.pickle\"\n",
    "            data.to_pickle(save_path)\n",
    "            # print(\"predictions saved to\", save_path)\n",
    "            # display(data)\n",
    "            \n",
    "            tmp = data\n",
    "            y_test = pd.concat(\n",
    "                [x for (i,x) in tmp[[\"fold\",\"y_test\"]].explode(column=\"y_test\").groupby(\"fold\")],\n",
    "                axis=0\n",
    "            ).reset_index(drop=True)\n",
    "            y_pred = pd.concat(\n",
    "                [x for (i,x) in tmp[[\"fold\",\"y_pred\"]].explode(column=\"y_pred\").groupby(\"fold\")],\n",
    "                axis=0\n",
    "            ).reset_index(drop=True)\n",
    "            test_index = pd.DataFrame(\n",
    "                sum(tmp.X_test.apply(lambda d: d.index.tolist()).values.tolist(), []),\n",
    "                columns=[\"sample_idx\"]\n",
    "            )\n",
    "            tmp2 = pd.concat(( y_test, y_pred[\"y_pred\"], test_index), axis=1)\n",
    "            tmp2.to_csv(save_path.replace(\"pickle\", \"csv\"), index=None)\n",
    "\n",
    "            data[\"MSE\"] = data.apply(lambda row: mean_squared_error(row.y_test, row.y_pred), axis=1)\n",
    "            data[\"R2\"] = data.apply(lambda row: r2_score(row.y_test, row.y_pred), axis=1)\n",
    "            data[\"MAPE\"] = data.apply(lambda row: mean_absolute_percentage_error(row.y_test, row.y_pred), axis=1)\n",
    "            data[\"RMSE\"] = data.apply(lambda row: root_mean_squared_error(row.y_test, row.y_pred), axis=1)\n",
    "            data[\"NRMSE\"] = data.apply(lambda row: normalized_root_mean_squared_error(row.y_test, row.y_pred, norm_factor=row.y_train.mean()), axis=1)\n",
    "\n",
    "            data = data.drop(columns=[\"X_train\", \"y_train\", \"X_test\", \"y_test\", \"y_pred\"])\n",
    "            save_path = f\"../results/metrics--{TARGET}--{CONFIG.name}--{model2paper[regressor.__class__.__name__]}.pickle\"\n",
    "            data.to_pickle(save_path)\n",
    "            print(\"metrics, predictions and models saved to\\n\", save_path)\n",
    "            # display(data)\n",
    "\n",
    "            for metric in [\"R2\", \"MAPE\", \"RMSE\", \"NRMSE\"]:\n",
    "\n",
    "                print(f\"{metric:>10} {data[metric].mean().round(2):>7} ± {data[metric].std().round(2):>5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea917f79-fcad-4c1e-9aeb-47dfddce7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(\"catboost_info\"):\n",
    "    shutil.rmtree(\"catboost_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c26af98-8c38-41cf-b6db-a352488baccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.268355698066373"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aece522-bf58-4c56-a523-37bdea032f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00601583, 0.01168756, 0.01090784, 0.00564513, 0.01117311,\n",
       "        0.01077352, 0.00626607, 0.01002245, 0.00951977]),\n",
       " 'std_fit_time': array([0.00131799, 0.00092005, 0.00096347, 0.00011742, 0.00065744,\n",
       "        0.00095156, 0.00108486, 0.00136825, 0.00125998]),\n",
       " 'mean_score_time': array([0.00124946, 0.00103912, 0.00101395, 0.00103679, 0.00109739,\n",
       "        0.00105276, 0.00111237, 0.00106645, 0.00080066]),\n",
       " 'std_score_time': array([1.44437478e-04, 1.01349783e-04, 3.58686153e-05, 2.39358955e-05,\n",
       "        7.89801839e-05, 1.16295646e-04, 7.13649536e-05, 3.77394847e-05,\n",
       "        2.01780640e-04]),\n",
       " 'param_kernel': masked_array(data=[None, RBF(length_scale=1), RBF(length_scale=0.5), None,\n",
       "                    RBF(length_scale=1), RBF(length_scale=0.5), None,\n",
       "                    RBF(length_scale=1), RBF(length_scale=0.5)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_alpha': masked_array(data=[1e-10, 1e-10, 1e-10, 1e-05, 1e-05, 1e-05, 1e-15, 1e-15,\n",
       "                    1e-15],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'kernel': None, 'alpha': 1e-10},\n",
       "  {'kernel': RBF(length_scale=1), 'alpha': 1e-10},\n",
       "  {'kernel': RBF(length_scale=0.5), 'alpha': 1e-10},\n",
       "  {'kernel': None, 'alpha': 1e-05},\n",
       "  {'kernel': RBF(length_scale=1), 'alpha': 1e-05},\n",
       "  {'kernel': RBF(length_scale=0.5), 'alpha': 1e-05},\n",
       "  {'kernel': None, 'alpha': 1e-15},\n",
       "  {'kernel': RBF(length_scale=1), 'alpha': 1e-15},\n",
       "  {'kernel': RBF(length_scale=0.5), 'alpha': 1e-15}],\n",
       " 'split0_test_score': array([-5.38445763, -5.38445763, -5.38445763, -5.38445763, -5.38445763,\n",
       "        -5.38445763, -5.38445763, -5.38445763, -5.38445763]),\n",
       " 'split1_test_score': array([-3.00928675, -3.00928675, -3.00928675, -3.00928675, -3.00928675,\n",
       "        -3.00928675, -3.00928675, -3.00928675, -3.00928675]),\n",
       " 'split2_test_score': array([-4.8921997, -4.8921997, -4.8921997, -4.8921997, -4.8921997,\n",
       "        -4.8921997, -4.8921997, -4.8921997, -4.8921997]),\n",
       " 'split3_test_score': array([-6.26319606, -6.26319606, -6.26319606, -6.26319606, -6.26319606,\n",
       "        -6.26319606, -6.26319606, -6.26319606, -6.26319606]),\n",
       " 'split4_test_score': array([-1.79263834, -1.79263834, -1.79263834, -1.79263834, -1.79263834,\n",
       "        -1.79263834, -1.79263834, -1.79263834, -1.79263834]),\n",
       " 'mean_test_score': array([-4.2683557, -4.2683557, -4.2683557, -4.2683557, -4.2683557,\n",
       "        -4.2683557, -4.2683557, -4.2683557, -4.2683557]),\n",
       " 'std_test_score': array([1.63270842, 1.63270842, 1.63270842, 1.63270842, 1.63270842,\n",
       "        1.63270842, 1.63270842, 1.63270842, 1.63270842]),\n",
       " 'rank_test_score': array([1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 'split0_train_score': array([-3.99918176e-20, -3.99918176e-20, -3.99918176e-20, -3.99910087e-10,\n",
       "        -3.99910087e-10, -3.99910087e-10, -3.13229775e-30, -3.13229775e-30,\n",
       "        -3.13229775e-30]),\n",
       " 'split1_train_score': array([-4.59563768e-20, -4.59563768e-20, -4.59563768e-20, -4.59554441e-10,\n",
       "        -4.59554441e-10, -4.59554441e-10, -3.63879142e-30, -3.63879142e-30,\n",
       "        -3.63879142e-30]),\n",
       " 'split2_train_score': array([-4.12279885e-20, -4.12279885e-20, -4.12279885e-20, -4.12271474e-10,\n",
       "        -4.12271474e-10, -4.12271474e-10, -3.27863388e-30, -3.27863388e-30,\n",
       "        -3.27863388e-30]),\n",
       " 'split3_train_score': array([-3.77851196e-20, -3.77851196e-20, -3.77851196e-20, -3.77843554e-10,\n",
       "        -3.77843554e-10, -3.77843554e-10, -2.91172712e-30, -2.91172712e-30,\n",
       "        -2.91172712e-30]),\n",
       " 'split4_train_score': array([-4.88728666e-20, -4.88728666e-20, -4.88728666e-20, -4.88718729e-10,\n",
       "        -4.88718729e-10, -4.88718729e-10, -3.88595471e-30, -3.88595471e-30,\n",
       "        -3.88595471e-30]),\n",
       " 'mean_train_score': array([-4.27668338e-20, -4.27668338e-20, -4.27668338e-20, -4.27659657e-10,\n",
       "        -4.27659657e-10, -4.27659657e-10, -3.36948098e-30, -3.36948098e-30,\n",
       "        -3.36948098e-30]),\n",
       " 'std_train_score': array([4.05815371e-21, 4.05815371e-21, 4.05815371e-21, 4.05807010e-11,\n",
       "        4.05807010e-11, 4.05807010e-11, 3.50234464e-31, 3.50234464e-31,\n",
       "        3.50234464e-31])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9826aa08-55b3-4006-b37d-5914332e7c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomizedSearchCV in module sklearn.model_selection._search:\n",
      "\n",
      "class RandomizedSearchCV(BaseSearchCV)\n",
      " |  RandomizedSearchCV(estimator, param_distributions, *, n_iter=10, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score=nan, return_train_score=False)\n",
      " |  \n",
      " |  Randomized search on hyper parameters.\n",
      " |  \n",
      " |  RandomizedSearchCV implements a \"fit\" and a \"score\" method.\n",
      " |  It also implements \"score_samples\", \"predict\", \"predict_proba\",\n",
      " |  \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n",
      " |  implemented in the estimator used.\n",
      " |  \n",
      " |  The parameters of the estimator used to apply these methods are optimized\n",
      " |  by cross-validated search over parameter settings.\n",
      " |  \n",
      " |  In contrast to GridSearchCV, not all parameter values are tried out, but\n",
      " |  rather a fixed number of parameter settings is sampled from the specified\n",
      " |  distributions. The number of parameter settings that are tried is\n",
      " |  given by n_iter.\n",
      " |  \n",
      " |  If all parameters are presented as a list,\n",
      " |  sampling without replacement is performed. If at least one parameter\n",
      " |  is given as a distribution, sampling with replacement is used.\n",
      " |  It is highly recommended to use continuous distributions for continuous\n",
      " |  parameters.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <randomized_parameter_search>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.14\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : estimator object.\n",
      " |      A object of that type is instantiated for each grid point.\n",
      " |      This is assumed to implement the scikit-learn estimator interface.\n",
      " |      Either estimator needs to provide a ``score`` function,\n",
      " |      or ``scoring`` must be passed.\n",
      " |  \n",
      " |  param_distributions : dict or list of dicts\n",
      " |      Dictionary with parameters names (`str`) as keys and distributions\n",
      " |      or lists of parameters to try. Distributions must provide a ``rvs``\n",
      " |      method for sampling (such as those from scipy.stats.distributions).\n",
      " |      If a list is given, it is sampled uniformly.\n",
      " |      If a list of dicts is given, first a dict is sampled uniformly, and\n",
      " |      then a parameter is sampled using that dict as above.\n",
      " |  \n",
      " |  n_iter : int, default=10\n",
      " |      Number of parameter settings that are sampled. n_iter trades\n",
      " |      off runtime vs quality of the solution.\n",
      " |  \n",
      " |  scoring : str, callable, list/tuple or dict, default=None\n",
      " |      A single str (see :ref:`scoring_parameter`) or a callable\n",
      " |      (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
      " |  \n",
      " |      For evaluating multiple metrics, either give a list of (unique) strings\n",
      " |      or a dict with names as keys and callables as values.\n",
      " |  \n",
      " |      NOTE that when using custom scorers, each scorer should return a single\n",
      " |      value. Metric functions returning a list/array of values can be wrapped\n",
      " |      into multiple scorers that return one value each.\n",
      " |  \n",
      " |      See :ref:`multimetric_grid_search` for an example.\n",
      " |  \n",
      " |      If None, the estimator's score method is used.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of jobs to run in parallel.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |      .. versionchanged:: v0.20\n",
      " |         `n_jobs` default changed from 1 to None\n",
      " |  \n",
      " |  pre_dispatch : int, or str, default=None\n",
      " |      Controls the number of jobs that get dispatched during parallel\n",
      " |      execution. Reducing this number can be useful to avoid an\n",
      " |      explosion of memory consumption when more jobs get dispatched\n",
      " |      than CPUs can process. This parameter can be:\n",
      " |  \n",
      " |          - None, in which case all the jobs are immediately\n",
      " |            created and spawned. Use this for lightweight and\n",
      " |            fast-running jobs, to avoid delays due to on-demand\n",
      " |            spawning of the jobs\n",
      " |  \n",
      " |          - An int, giving the exact number of total jobs that are\n",
      " |            spawned\n",
      " |  \n",
      " |          - A str, giving an expression as a function of n_jobs,\n",
      " |            as in '2*n_jobs'\n",
      " |  \n",
      " |  cv : int, cross-validation generator or an iterable, default=None\n",
      " |      Determines the cross-validation splitting strategy.\n",
      " |      Possible inputs for cv are:\n",
      " |  \n",
      " |      - None, to use the default 5-fold cross validation,\n",
      " |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      " |      - :term:`CV splitter`,\n",
      " |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |  \n",
      " |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      " |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      " |      other cases, :class:`KFold` is used.\n",
      " |  \n",
      " |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      " |      cross-validation strategies that can be used here.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      " |  \n",
      " |  refit : bool, str, or callable, default=True\n",
      " |      Refit an estimator using the best found parameters on the whole\n",
      " |      dataset.\n",
      " |  \n",
      " |      For multiple metric evaluation, this needs to be a `str` denoting the\n",
      " |      scorer that would be used to find the best parameters for refitting\n",
      " |      the estimator at the end.\n",
      " |  \n",
      " |      Where there are considerations other than maximum score in\n",
      " |      choosing a best estimator, ``refit`` can be set to a function which\n",
      " |      returns the selected ``best_index_`` given the ``cv_results``. In that\n",
      " |      case, the ``best_estimator_`` and ``best_params_`` will be set\n",
      " |      according to the returned ``best_index_`` while the ``best_score_``\n",
      " |      attribute will not be available.\n",
      " |  \n",
      " |      The refitted estimator is made available at the ``best_estimator_``\n",
      " |      attribute and permits using ``predict`` directly on this\n",
      " |      ``RandomizedSearchCV`` instance.\n",
      " |  \n",
      " |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      " |      ``best_score_`` and ``best_params_`` will only be available if\n",
      " |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      " |      scorer.\n",
      " |  \n",
      " |      See ``scoring`` parameter to know more about multiple metric\n",
      " |      evaluation.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |          Support for callable added.\n",
      " |  \n",
      " |  verbose : int\n",
      " |      Controls the verbosity: the higher, the more messages.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Pseudo random number generator state used for random uniform sampling\n",
      " |      from lists of possible values instead of scipy.stats distributions.\n",
      " |      Pass an int for reproducible output across multiple\n",
      " |      function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  error_score : 'raise' or numeric, default=np.nan\n",
      " |      Value to assign to the score if an error occurs in estimator fitting.\n",
      " |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      " |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      " |      step, which will always raise the error.\n",
      " |  \n",
      " |  return_train_score : bool, default=False\n",
      " |      If ``False``, the ``cv_results_`` attribute will not include training\n",
      " |      scores.\n",
      " |      Computing training scores is used to get insights on how different\n",
      " |      parameter settings impact the overfitting/underfitting trade-off.\n",
      " |      However computing the scores on the training set can be computationally\n",
      " |      expensive and is not strictly required to select the parameters that\n",
      " |      yield the best generalization performance.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |          Default value was changed from ``True`` to ``False``\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cv_results_ : dict of numpy (masked) ndarrays\n",
      " |      A dict with keys as column headers and values as columns, that can be\n",
      " |      imported into a pandas ``DataFrame``.\n",
      " |  \n",
      " |      For instance the below given table\n",
      " |  \n",
      " |      +--------------+-------------+-------------------+---+---------------+\n",
      " |      | param_kernel | param_gamma | split0_test_score |...|rank_test_score|\n",
      " |      +==============+=============+===================+===+===============+\n",
      " |      |    'rbf'     |     0.1     |       0.80        |...|       1       |\n",
      " |      +--------------+-------------+-------------------+---+---------------+\n",
      " |      |    'rbf'     |     0.2     |       0.84        |...|       3       |\n",
      " |      +--------------+-------------+-------------------+---+---------------+\n",
      " |      |    'rbf'     |     0.3     |       0.70        |...|       2       |\n",
      " |      +--------------+-------------+-------------------+---+---------------+\n",
      " |  \n",
      " |      will be represented by a ``cv_results_`` dict of::\n",
      " |  \n",
      " |          {\n",
      " |          'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],\n",
      " |                                        mask = False),\n",
      " |          'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),\n",
      " |          'split0_test_score'  : [0.80, 0.84, 0.70],\n",
      " |          'split1_test_score'  : [0.82, 0.50, 0.70],\n",
      " |          'mean_test_score'    : [0.81, 0.67, 0.70],\n",
      " |          'std_test_score'     : [0.01, 0.24, 0.00],\n",
      " |          'rank_test_score'    : [1, 3, 2],\n",
      " |          'split0_train_score' : [0.80, 0.92, 0.70],\n",
      " |          'split1_train_score' : [0.82, 0.55, 0.70],\n",
      " |          'mean_train_score'   : [0.81, 0.74, 0.70],\n",
      " |          'std_train_score'    : [0.01, 0.19, 0.00],\n",
      " |          'mean_fit_time'      : [0.73, 0.63, 0.43],\n",
      " |          'std_fit_time'       : [0.01, 0.02, 0.01],\n",
      " |          'mean_score_time'    : [0.01, 0.06, 0.04],\n",
      " |          'std_score_time'     : [0.00, 0.00, 0.00],\n",
      " |          'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],\n",
      " |          }\n",
      " |  \n",
      " |      NOTE\n",
      " |  \n",
      " |      The key ``'params'`` is used to store a list of parameter\n",
      " |      settings dicts for all the parameter candidates.\n",
      " |  \n",
      " |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      " |      ``std_score_time`` are all in seconds.\n",
      " |  \n",
      " |      For multi-metric evaluation, the scores for all the scorers are\n",
      " |      available in the ``cv_results_`` dict at the keys ending with that\n",
      " |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      " |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      " |  \n",
      " |  best_estimator_ : estimator\n",
      " |      Estimator that was chosen by the search, i.e. estimator\n",
      " |      which gave highest score (or smallest loss if specified)\n",
      " |      on the left out data. Not available if ``refit=False``.\n",
      " |  \n",
      " |      For multi-metric evaluation, this attribute is present only if\n",
      " |      ``refit`` is specified.\n",
      " |  \n",
      " |      See ``refit`` parameter for more information on allowed values.\n",
      " |  \n",
      " |  best_score_ : float\n",
      " |      Mean cross-validated score of the best_estimator.\n",
      " |  \n",
      " |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      " |      ``False``. See ``refit`` parameter for more information.\n",
      " |  \n",
      " |      This attribute is not available if ``refit`` is a function.\n",
      " |  \n",
      " |  best_params_ : dict\n",
      " |      Parameter setting that gave the best results on the hold out data.\n",
      " |  \n",
      " |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      " |      ``False``. See ``refit`` parameter for more information.\n",
      " |  \n",
      " |  best_index_ : int\n",
      " |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      " |      candidate parameter setting.\n",
      " |  \n",
      " |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      " |      the parameter setting for the best model, that gives the highest\n",
      " |      mean score (``search.best_score_``).\n",
      " |  \n",
      " |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      " |      ``False``. See ``refit`` parameter for more information.\n",
      " |  \n",
      " |  scorer_ : function or a dict\n",
      " |      Scorer function used on the held out data to choose the best\n",
      " |      parameters for the model.\n",
      " |  \n",
      " |      For multi-metric evaluation, this attribute holds the validated\n",
      " |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      " |  \n",
      " |  n_splits_ : int\n",
      " |      The number of cross-validation splits (folds/iterations).\n",
      " |  \n",
      " |  refit_time_ : float\n",
      " |      Seconds used for refitting the best model on the whole dataset.\n",
      " |  \n",
      " |      This is present only if ``refit`` is not False.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  multimetric_ : bool\n",
      " |      Whether or not the scorers compute several metrics.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The parameters selected are those that maximize the score of the held-out\n",
      " |  data, according to the scoring parameter.\n",
      " |  \n",
      " |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      " |  parameter setting(and not `n_jobs` times). This is done for efficiency\n",
      " |  reasons if individual jobs take very little time, but may raise errors if\n",
      " |  the dataset is large and not enough memory is available.  A workaround in\n",
      " |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      " |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      " |  n_jobs`.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  GridSearchCV : Does exhaustive search over a grid of parameters.\n",
      " |  ParameterSampler : A generator over parameter settings, constructed from\n",
      " |      param_distributions.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> from sklearn.model_selection import RandomizedSearchCV\n",
      " |  >>> from scipy.stats import uniform\n",
      " |  >>> iris = load_iris()\n",
      " |  >>> logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
      " |  ...                               random_state=0)\n",
      " |  >>> distributions = dict(C=uniform(loc=0, scale=4),\n",
      " |  ...                      penalty=['l2', 'l1'])\n",
      " |  >>> clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
      " |  >>> search = clf.fit(iris.data, iris.target)\n",
      " |  >>> search.best_params_\n",
      " |  {'C': 2..., 'penalty': 'l1'}\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomizedSearchCV\n",
      " |      BaseSearchCV\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, param_distributions, *, n_iter=10, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score=nan, return_train_score=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSearchCV:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Call decision_function on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``decision_function``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  fit(self, X, y=None, *, groups=None, **fit_params)\n",
      " |      Run fit with all sets of parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      groups : array-like of shape (n_samples,), default=None\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      " |          instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n",
      " |      \n",
      " |      **fit_params : dict of str -> object\n",
      " |          Parameters passed to the ``fit`` method of the estimator\n",
      " |  \n",
      " |  inverse_transform(self, Xt)\n",
      " |      Call inverse_transform on the estimator with the best found params.\n",
      " |      \n",
      " |      Only available if the underlying estimator implements\n",
      " |      ``inverse_transform`` and ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Xt : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Call predict on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Call predict_log_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_log_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Call predict_proba on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``predict_proba``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Returns the score on the given data, if the estimator has been refit.\n",
      " |      \n",
      " |      This uses the score defined by ``scoring`` where provided, and the\n",
      " |      ``best_estimator_.score`` method otherwise.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      " |          Target relative to X for classification or regression;\n",
      " |          None for unsupervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |  \n",
      " |  score_samples(self, X)\n",
      " |      Call score_samples on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if ``refit=True`` and the underlying estimator supports\n",
      " |      ``score_samples``.\n",
      " |      \n",
      " |      .. versionadded:: 0.24\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : iterable\n",
      " |          Data to predict on. Must fulfill input requirements\n",
      " |          of the underlying estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_score : ndarray of shape (n_samples,)\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Call transform on the estimator with the best found parameters.\n",
      " |      \n",
      " |      Only available if the underlying estimator supports ``transform`` and\n",
      " |      ``refit=True``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : indexable, length n_samples\n",
      " |          Must fulfill the input assumptions of the\n",
      " |          underlying estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseSearchCV:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  n_features_in_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomizedSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ce27a-e97e-4c4a-b6c4-27ae50c2e2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bp_base",
   "language": "python",
   "name": "bp_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
