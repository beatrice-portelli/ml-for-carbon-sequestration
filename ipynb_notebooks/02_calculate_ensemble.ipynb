{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa47fdd9-9cdf-485e-9b25-8df64963d139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 14:14:34.628925: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-08 14:14:34.868869: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-08-08 14:14:34.868886: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-08-08 14:14:35.949952: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-08 14:14:35.950119: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-08 14:14:35.950138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CS--Conf1--Ensemble.pickle\n",
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CS--Conf2--Ensemble.pickle\n",
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CS--Conf3--Ensemble.pickle\n",
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CS--Conf4--Ensemble.pickle\n",
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CSE--Conf1--Ensemble.pickle\n",
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CSE--Conf2--Ensemble.pickle\n",
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CSE--Conf3--Ensemble.pickle\n",
      "metrics, predictions and models saved to\n",
      " ../results/metrics--CSE--Conf4--Ensemble.pickle\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from experiments_to_run import *\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "\n",
    "def normalized_root_mean_squared_error(y_true, y_pred, norm_factor=None):\n",
    "    if norm_factor is None:\n",
    "        assert False, \"Set norm_factor (for example the average target value for the training set)\"\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    return (rmse / norm_factor)*100\n",
    "\n",
    "\n",
    "ENSEMBLE_MODELS = [\n",
    "    \"AdaBoost\",\n",
    "    \"CatBoost\",\n",
    "    \"RF\",\n",
    "    \"StackEns\",\n",
    "]\n",
    "\n",
    "ensemble_res = []\n",
    "\n",
    "for TARGET in TARGETS:\n",
    "    for CONFIG in CONFIGS:\n",
    "        \n",
    "        CONFIG = CONFIG.name\n",
    "        \n",
    "        root = \"../results\"\n",
    "        res = None\n",
    "        \n",
    "        for MODEL in ENSEMBLE_MODELS:\n",
    "\n",
    "            data = pd.read_pickle(f\"{root}/predictions--{TARGET}--{CONFIG}--{MODEL}.pickle\")\n",
    "            if res is None:\n",
    "                res = data[[\"y_pred\", \"y_pred_train\"]].copy()\n",
    "            else:\n",
    "                res += data[[\"y_pred\", \"y_pred_train\"]]\n",
    "\n",
    "        res = res/len(ENSEMBLE_MODELS)\n",
    "\n",
    "        for col_name in [\n",
    "            \"target\",\n",
    "            \"config\",\n",
    "            \"fold\",\n",
    "            \"X_train\",\n",
    "            \"y_train\",\n",
    "            \"X_test\",\n",
    "            \"y_test\",\n",
    "        ]:\n",
    "            res[col_name] = data[col_name]\n",
    "        res[\"model\"] = \"â–¸ Ensemble\"\n",
    "        res[\"model_name\"] = None\n",
    "        res[\"hyperparams\"] = None\n",
    "        res[\"model_obj\"] = None\n",
    "        \n",
    "        res = res[data.columns.tolist()]\n",
    "        \n",
    "        save_path = f\"{root}/predictions--{TARGET}--{CONFIG}--Ensemble.pickle\"\n",
    "        \n",
    "        res.to_pickle(save_path)\n",
    "        \n",
    "        tmp = res\n",
    "        y_test = pd.concat(\n",
    "            [x for (i,x) in tmp[[\"fold\",\"y_test\"]].explode(column=\"y_test\").groupby(\"fold\")],\n",
    "            axis=0\n",
    "        ).reset_index(drop=True)\n",
    "        y_pred = pd.concat(\n",
    "            [x for (i,x) in tmp[[\"fold\",\"y_pred\"]].explode(column=\"y_pred\").groupby(\"fold\")],\n",
    "            axis=0\n",
    "        ).reset_index(drop=True)\n",
    "        \n",
    "        y_train = pd.concat(\n",
    "            [x for (i,x) in tmp[[\"fold\",\"y_train\"]].explode(column=\"y_train\").groupby(\"fold\")],\n",
    "            axis=0\n",
    "        ).reset_index(drop=True)\n",
    "        y_pred_train = pd.concat(\n",
    "            [x for (i,x) in tmp[[\"fold\",\"y_pred_train\"]].explode(column=\"y_pred_train\").groupby(\"fold\")],\n",
    "            axis=0\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        test_index = pd.DataFrame(\n",
    "            sum(tmp.X_test.apply(lambda d: d.index.tolist()).values.tolist(), []),\n",
    "            columns=[\"sample_idx\"]\n",
    "        )\n",
    "        train_index = pd.DataFrame(\n",
    "            sum(tmp.X_train.apply(lambda d: d.index.tolist()).values.tolist(), []),\n",
    "            columns=[\"sample_idx\"]\n",
    "        )\n",
    "\n",
    "        \n",
    "        tmp2 = pd.concat(( y_test, y_pred[\"y_pred\"], test_index), axis=1)\n",
    "        tmp2.to_csv(save_path.replace(\"pickle\", \"csv\").replace(\"predictions\", \"predictions_test\"), index=None)\n",
    "        \n",
    "        tmp2 = pd.concat(( y_train, y_pred_train[\"y_pred_train\"], train_index), axis=1)\n",
    "        tmp2.to_csv(save_path.replace(\"pickle\", \"csv\").replace(\"predictions\", \"predictions_train\"), index=None)\n",
    "        \n",
    "        data = res\n",
    "        \n",
    "        data[\"MSE\"] = data.apply(lambda row: mean_squared_error(row.y_test, row.y_pred), axis=1)\n",
    "        data[\"R2\"] = data.apply(lambda row: r2_score(row.y_test, row.y_pred), axis=1)\n",
    "        data[\"MAPE\"] = data.apply(lambda row: mean_absolute_percentage_error(row.y_test, row.y_pred), axis=1)\n",
    "        data[\"RMSE\"] = data.apply(lambda row: root_mean_squared_error(row.y_test, row.y_pred), axis=1)\n",
    "        data[\"NRMSE\"] = data.apply(lambda row: normalized_root_mean_squared_error(row.y_test, row.y_pred, norm_factor=row.y_train.mean()), axis=1)\n",
    "        \n",
    "        data[\"MSE_train\"] = data.apply(lambda row: mean_squared_error(row.y_train, row.y_pred_train), axis=1)\n",
    "        data[\"R2_train\"] = data.apply(lambda row: r2_score(row.y_train, row.y_pred_train), axis=1)\n",
    "        data[\"MAPE_train\"] = data.apply(lambda row: mean_absolute_percentage_error(row.y_train, row.y_pred_train), axis=1)\n",
    "        data[\"RMSE_train\"] = data.apply(lambda row: root_mean_squared_error(row.y_train, row.y_pred_train), axis=1)\n",
    "        data[\"NRMSE_train\"] = data.apply(lambda row: normalized_root_mean_squared_error(row.y_train, row.y_pred_train, norm_factor=row.y_train.mean()), axis=1)\n",
    "\n",
    "        data = data.drop(columns=[\"X_train\", \"y_train\", \"X_test\", \"y_test\", \"y_pred\", \"y_pred_train\"])\n",
    "        save_path = f\"../results/metrics--{TARGET}--{CONFIG}--Ensemble.pickle\"\n",
    "        \n",
    "        data.to_pickle(save_path)\n",
    "        print(\"metrics, predictions and models saved to\\n\", save_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c78e4-fd84-40fe-94ee-6f4feb626aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bp_base",
   "language": "python",
   "name": "bp_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
